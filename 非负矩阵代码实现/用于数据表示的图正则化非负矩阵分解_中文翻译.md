###用于数据表示的图正则化非负矩阵分解



**摘要** 矩阵分解技术在信息检索、计算机视觉和模式识别等领域得到了广泛的应用。其中，非负矩阵因子(NMF)由于其对自然发生数据的心理和生理解释而受到了广泛的关注，其表征可能是基于人脑的部分。另一方面，从几何角度来看，数据通常是从高维环境空间中嵌入的低维流形进行采样的。然后，我们希望找到一个紧凑的表示，它揭示了隐藏的语义，同时尊重内在的几何结构。为此，本文提出了一种新的图正则化非负矩阵因子分解算法。在GNMF中，构造一个亲和图对几何信息进行编码，并寻求一种尊重图结构的矩阵分解。我们的实证研究表明，与最先进的算法相比，该算法在实际问题上取得了令人鼓舞的结果。

**索引项 ** 非负矩阵分解，图拉普拉斯分解，流形正则化，聚类。



**一，简介**

近年来，矩阵分解技术在数据表示中得到了广泛的应用。在信息检索、计算机视觉和模式识别等许多问题中，输入数据矩阵具有很高的维数。这使得从示例中学习[15]是不可行的。然后，我们希望找到两个或两个以上的低维矩阵，它们的乘积能很好地逼近原矩阵。典型矩阵分解技术包括LU分解、QR分解、矢量量化和奇异值分解(SVD)。
奇异值分解(SVD)是最常用的矩阵分解技术之一。M*􏰈N矩阵X的奇异值分解具有以下形式:![image-20190716191728151](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716191728151.png)

U是一个M􏰈\*M正交矩阵,V是一个N*􏰈N正交矩阵,􏰏是一个M\*􏰈N对角矩阵􏰏。这个数量称为X的奇异值, U和V的列被称为左和右奇异向量,分别。通过移除那些对应于足够小的奇异值的奇异向量，我们得到了原始矩阵的低秩近似。这种近似在重建误差方面是最优的，因此当考虑欧几里德结构时，数据表示也是最优的。基于此，SVD已被广泛应用于人脸识别(特征脸，[40])、神经网络(神经网络)、神经网络(神经网络)等各种实际应用中。

以往的研究表明，在人脑[34]、[41]、[31]中存在基于部分的表达的心理和生理证据。提出了一种非负矩阵分解(NMF)算法来学习人脸、文本文档[33]、[26]等对象的局部特征。NMF的目标是找到两个非负矩阵，它们的乘积能很好地逼近原矩阵。非负约束导致基于部件的表示，因为它们只允许加法组合，而不允许减法组合。在人脸识别[29]和文档聚类[42]方面，NMF优于SVD。它是学习物体部分的最佳方法。

最近，各种研究人员(见[39]、[35]、[1]、[36]、[2])都考虑过这样一种情况，即数据来自于采样的概率分布，该分布支持或接近环境空间的子流形。在这里,一个采用欧几里得空间IRM的子流形是一个子集Md􏰉IRM,看起来就像一个本地平d-dimen——sional欧几里得空间[28]。为了检测底层流形结构，提出了多种流形学习算法，如局部线性嵌入(LLE)[35]、ISOMAP[39]和拉普拉斯特征映射[1]。所有这些算法都使用了所谓的局部不变思想[18]，即，附近的点很可能有类似的嵌入。研究表明，利用几何结构，考虑局部不变性，可以显著提高学习性能。

摘要基于矩阵分解和流形学习[2]、[5]、[6]、[7]的最新进展，提出了一种考虑局部不变性的图正则化非负矩阵分解(GNMF)算法。通过构造最近邻图，对数据空间的几何信息进行编码。我们的目标是找到一个基于部件的表示空间，在这个空间中，如果两个数据点在图中连接在一起，那么它们就足够接近。为此，我们设计了一个新的矩阵因子分解目标函数，并将其与图结构进行了不匹配。提出了一种基于两因子矩阵迭代更新求解目标函数的优化方案。这导致了一种新的基于零件的数据表示方法，它尊重数据空间的几何结构。给出了优化方案的收敛性证明。

值得在此强调拟议方法的几个方面:

1. 虽然标准的NMF适合于欧几里德空间中的数据，但我们的算法利用了数据分布的内在几何特性，并将其作为一个额外的正则项合并在一起。因此，我们的算法特别适用于从嵌入高维环境空间的子流形中采样数据。
2. 我们的算法构造了一个最近邻图来对流形结构建模。图的权矩阵是高度稀疏的。因此，针对GNMF的多重更新规则是非常有效的。通过保持图的结构，我们的算法比标准的NMF算法具有更强的识别力。

3. 最近的[17]、[13]研究表明，NMF与概率潜在语义分析(PLSA)[21]密切相关。后者是最流行的主题建模算法之一。具体地说，具有kl发散公式的NMF等价于PLSA[13]。从这个角度看，本文提出的GNMF方法也为将几何结构融入主题建模提供了一种原则性的方法。

4. 所提出的框架是一种通用的框架，可以同时利用NMF和图拉普拉斯正则化的能力。除了最近邻信息外，关于数据的其他知识(如标签信息、社交网络结构)也可以用来构建图表。这自然会导致其他扩展(例如，半监督NMF)。

本文的其余部分组织如下:在第二节中，我们简要回顾了NMF。第三节介绍了我们的算法，并给出了优化方案的收敛性证明。第4节给出了关于聚类的大量实验结果。最后，在第五部分中，我们对今后的工作提出了一些结束语和建议。



**2 NMF概述**

NMF[26]是一种矩阵分解算法，它主要分析非负元素的数据矩阵。
给定一个数据矩阵X 1⁄4 1⁄2 x1;……; 2 xN􏰊IRM􏰈N, X是一个样本向量的每一列。NMF旨在找到两个非负矩阵U1⁄41⁄2 uik􏰊2IRM􏰈K V1⁄41⁄2 vjk􏰊2写作􏰈K的产品可以近似原始矩阵X:
X􏰋UVT:
有两个常用的成本函数来量化近似的质量。第一个是两个矩阵之间的欧氏距离的平方(两个矩阵差的弗洛贝尼乌斯范数的平方)[33]:
2 !
第二个是两者之间的“散度”
矩阵[27]:
X􏰒􏰓
O2 1⁄4 dðxkuvtÞ1⁄4 xijlog ij􏰌xijþyij;ð2Þ
Y 1⁄4 1⁄2 yij􏰊1⁄4 UVT。这个代价函数被称为X与Y的散度，而不是X与Y之间的距离，因为它不是对称的。换句话说,DðXkYÞ61⁄4 DðYkXÞ。当Pij xij 1 / 4 / Pij yij 1 / 4射入射孔后，射孔射孔率降低为kullbackleibler散度或相对熵，因此X和Y可以看作是归一化的概率分布。在本文的其余部分，我们将O1作为f -范数公式，O2作为散度公式。
虽然目标函数O1 in(1)和O2 in(2)仅在U或V上是凸的，但它们在两个变量上同时不是凸的。因此，期望算法找到O1(或O2)的全局最小值是不现实的。Lee和Seung[27]提出了两种迭代更新算法。(1)中目标函数O1的最小化算法如下:
ðXT UÞjk
X XK
O1 1⁄4 kx􏰌UVTk2 1⁄4 xij􏰌uikvjk
:ð1Þ
uik
vjk
vjkðVUTUÞ
:
ðXVÞik uikðUVTVÞ
;
(2)中目标函数O2的最小化算法为
P􏰌P􏰇
j xijvjk= k uikvjk uikuik Pv;
j jk
π􏰌xijuik = Pk uikvjk􏰇vjk vjk菩:
我本土知识
证明了上述两种算法都能找到目标函数O1和O2[27]的局部极小值。
在现实中,我们有K􏰍<<M和K<<􏰍N。因此，NMF本质上试图找到原始数据矩阵的压缩近似。我们可以把这个近似一列一列地看成
XK k1⁄41
其中uk为U的第k列向量。因此,每个数据向量xj是近似的线性组合的列U,加权的组件v .因此,U可以被视为包含基础上,也就是说,优化数据的线性近似的x。让zTj表示V的第j行, zj 1⁄41⁄2 vj1;……; vjk􏰊T。zj j的可以被视为新的表示数据点对U新的基础。由于相对较少的基向量用于表示许多数据向量,一个好的近似只有通过基向量发现隐藏在数据结构[27]。
U和V上的非负约束只允许不同碱基之间的加法组合。这是NMF与其他矩阵分解方法(如SVD)最显著的区别。与SVD不同，NMF中不存在减法。因此，我们认为NMF可以学习一种基于部件的表示[26]。在人脸分析、文档聚类和DNA基因表达分析等实际问题中，已经发现了这种基于部分的表示方法的优点。

**3 图的正则化非负矩阵分解**

通过使用非负约束，NMF可以学习基于部件的表示。然而，NMF在欧几里德空间中执行这种学习。它没有发现数据空间固有的几何和判别结构，而这对实际应用是至关重要的。在本节中，我们介绍了我们的GNMF算法，它通过结合一个基于几何的正则化器来避免这一限制。
3.1带有流形正则化的NMF
回想一下，NMF试图找到一组基向量，可以用来最好地近似数据。人们可能还希望基向量能够尊重固有的黎曼结构，而不是周围的欧氏结构。一个自然的假设是如果两个数据点xj;xl在数据分布的固有几何关系上很接近，那么zj和zl，这两个点相对于新的基底的表示，也很接近。这种假设通常被称为局部不变性假设[1]、[19]、[7]，它在各种算法的发展中发挥着至关重要的作用，包括降维算法[1]和半监督学习算法[2]、[46]、[45]。

近年来，谱图理论[9]和流形学习理论[1]的研究表明，通过离散数据点的最近邻图可以有效地建模局部几何结构。考虑一个有N个顶点的图，其中每个顶点对应一个数据点。对于每个数据点xj，我们找到它的p个最近邻居，并在xj和它的邻居之间放置边。定义图上的权矩阵W有很多选择。最常用的三种方法如下:

1. 0 - 1权重。1 / 4 1，当且仅当节点j和l由一条边连接。这是最简单的加权方法，很容易计算。
2. 热内核权重。如果节点j和l是连通的，则在流形[1]上的可微函数上，put热核与拉普拉斯-贝尔特拉米算子具有内在的联系。![image-20190716192807635](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716192807635.png)

3. 点积权重。如果节点j和l相连，注意如果x被归一化为1，两个向量的点积等于两个向量的余弦相似度。![image-20190716192928458](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716192928458.png)

Wjl用于测量两个点xj和xl的接近度。不同的相似性度量方法适用于不同的情况。例如，余弦相似度(点积权重)在IR社区中非常流行(用于处理文档)，而对于图像数据，热核权重可能是更好的选择。由于本文中的Wjl仅用于度量贴近度，所以我们没有对不同的权重方案进行单独处理。

xj相对于新基的低维表示是zj。同样，我们可以用欧几里德距离![image-20190716193013847](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716193013847.png)

或散度，以衡量两个数据点相对于新基的低二值表示之间的“不同之处”。![image-20190716193040571](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716193040571.png)

与上述定义权重矩阵W,我们可以用以下两个方面来衡量的平滑的低维表示Trð􏰇Þ表示的跟踪矩阵和D是一个对角矩阵的条目列(或行,因为W 是对称的)W的和; Djj 1⁄4 plwjl。L1⁄4 d􏰌W,叫做图拉普拉斯算子[9]。![image-20190716193139373](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716193139373.png)

通过最小化R1(或R2)，我们期望如果两个数据点xj和xl很接近(即， Wjl很大)，zj和zl也很接近。将这种基于几何的正则化器与原始的NMF目标函数相结合，就得到了我们的GNMF。

给定一个数据矩阵X 1⁄4 1⁄2 xij􏰊2 IRM􏰈N, GNMF旨在找到两个非负矩阵U 1⁄4 1⁄2 uik􏰊2 IRM􏰈K和V 1⁄4 1⁄2 vjk􏰊2写作􏰈K。与NMF类似，我们也可以在这里使用两个“距离”度量。如果使用欧氏距离，则GNMF将目标函数最小化如下:![image-20190716193324426](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716193324426.png)

如果使用散度，则GNMF最小化正则化参数r>0控制新表示的平滑性的地方。![image-20190716193642662](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716193642662.png)

3.2更新规则最小化(6)

(6)和(7)中GNMF的目标函数O1和O2在U和V中同时不凸。因此，期望算法能找到全局极小值是不现实的。在接下来的文章中，我们介绍了两种迭代算法，它们都可以实现局部极小值。

我们首先讨论如何最小化目标函数O1，它可以重写为![image-20190716193742274](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716193742274.png)

第二个平等适用于矩阵属性TrðABÞ1⁄4 TrðBAÞTrðAÞ1⁄4 TrðATÞ。让本土知识和􏰂jk是约束的拉格朗日乘子uik􏰁0和vjk􏰁0,分别和􏰐1⁄4 1⁄2翼􏰊􏰑1⁄4 1⁄2􏰂jk􏰊,拉格朗日L![image-20190716193805997](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716193805997.png)

L对U和V的偏导是![image-20190716193831764](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716193831764.png)

使用马条件ikuik 1⁄4 0和􏰂jkvjk 1⁄4 0,我们得到以下方程uik vjk:![image-20190716193906760](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716193906760.png)

这些方程的更新规则如下:![image-20190716193932553](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716193932553.png)对于这两条更新规则，我们有如下定理:

定理1。(6)中的目标函数O1在(14)和(15)中的更新规则下不增加。
以上定理的详细证明见附录。我们的证明本质上遵循李承宪和承宪对原始NMF的[27]论文的证明思想。最近的[8]、[30]研究表明，Lee和Seung的[27]乘法算法不能保证收敛到一个平稳点。特别地，Lin[30]建议对Lee和Seung的算法做一些小的修改，这样可以收敛。(14)和(15)中的更新规则与NMF的更新规则基本相似，因此Lin的修改也可以应用。
当􏰁1⁄4 0时,很容易检查更新规则(14)和(15)减少原始NMF的更新规则。

对于NMF的目标函数，很容易判断U和V是否为解，则为UD;VD􏰌1也将形成一个解决方案的任何积极的对角矩阵d .消除这种不确定性,在实践中,人们将进一步要求每列向量矩阵的欧几里得距离U或V)是1 [42]。矩阵V(或U)将相应地进行调整，使UVT不变。我们的GNMF也采用了这一策略。在多次更新过程收敛后，我们将矩阵U中每个列向量的欧氏长度设为1，并调整矩阵V使UVT不变。![image-20190716194020907](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194020907.png)

3.3梯度下降法的连接
求(6)中GNMF目标函数最小值的另一种通用算法是梯度下降[25]。对于我们的问题，梯度下降导致了以下加法更新规则:![image-20190716194127597](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194127597.png)

􏰃ik和􏰄jk通常称为步长参数。只要􏰃ik􏰄jk足够小,上面的更新应该减少O1除非U和V驻点。
一般来说，在保持uik和vjk的非负性的同时设置这些步长参数是比较困难的。然而，由于偏导数的特殊形式，我们可以使用一些技巧来自动设置步长参数。让􏰃ik 1⁄4􏰌uik = 2ðuvt VÞik,![image-20190716194205153](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194205153.png)

*表:在NMF和GNMF中，计算操作对每次迭代计数*

![image-20190716194304959](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194304959.png)

*fladd:浮点加法，flmlt:浮点乘法，fldiv:浮点除法。N:样本点个数，M:特征个数，K:因子个数。p:最近邻数，q:共轭梯度(CG)迭代次数。*

同样,让􏰄jk 1⁄4􏰌vjk = 2ðvut Uþ􏰁DVÞjk,![image-20190716194417540](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194417540.png)

现在，很明显，(14)和(15)中的乘法更新规则是梯度下降的特殊情况，具有自动步长参数选择。乘法更新规则的优点是保证了U和v的非负性。定理1还保证(14)和(15)中的乘法更新规则收敛到局部最优。

3.4更新规则最小化(7)
对于GNMF的散度公式，我们也有两个更新规则，可以达到局部最小值(7):![image-20190716194526331](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194526331.png)
vk是V的k列和I是一个N􏰈*N单位矩阵。
类似地，我们有以下定理:
定理2。(7)中的目标函数O不随(20)和(21)中的更新规则而增加。当且仅当U和V在一个固定点时，目标函数在这些更新下是不变的。

详细证明见附件。本节的更新规则(最小化(7)的散度公式)与第3.2节的更新规则(最小化F-norm公式)不同。对于NMF的散度公式，前人的研究[16]成功地从EM算法的极大似然角度分析了乘法算法[27]的收敛性。这种分析在GNMF案例中也是有效的。

􏰁1⁄4 0时,很容易检查更新规则(20)和(21)减少原始NMF的更新规则。

3.5计算复杂度分析
在本节中，我们将讨论与标准NMF相比，我们提出的算法的额外计算成本。特别地，我们为f -范数和kl -散度公式提供了GNMF的计算复杂度分析。
表示算法复杂度的常用方法是使用大O符号[10]。但是，这还不够精确，无法区分GNMF和NMF的复杂性。因此，我们计算每个算法的算术运算。
根据更新规则，计算NMF中每次迭代的算术运算并不困难。我们将结果总结在表1中。对于GNMF，需要注意的是W是一个稀疏矩阵。如果我们使用p-近邻图，那么W每一行的平均非零元素是p。因此，我们只需要NpK flam(浮点加法和乘法)来计算WV。我们还总结了表1中GNMF的算术运算。
更新规则(21)与散度公式涉及GNMF大矩阵求逆矩阵πuikIþ􏰁L。实际上，不需要实际计算反演。我们只需要解线性方程组如下:![image-20190716194726955](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194726955.png)

由于矩阵是对称的，正定的，稀疏的，我们可以使用迭代算法CG[20]来解决这个线性方程组非常有效。在每个迭代中,形式的CGnePedstocomputethematrix-vectorproductsð我uikIþ􏰁LÞp。CG在每次迭代中剩余的工作负载为4N flam。因此,CG的时间成本在每个迭代中pNþ4 n。如果CG停止问迭代后,总时间成本是qðpþ4Þn。CG收敛非常快，通常在20次迭代内。因为我们需要解决K线性方程系统的总时间成本是qðpþ4Þnk。![image-20190716194831994](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194831994.png)

除了乘法更新,GNMF也需要OðN 2 MÞ构建p-nearest邻居图。假设迭代t之后乘法更新停止，那么NMF(两种公式)的总成本为![image-20190716194900640](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194900640.png)

采用f -范数公式计算GNMF的总成本为![image-20190716194919994](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194919994.png)

而采用散度公式的GNMF的代价为![image-20190716194937601](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716194937601.png)

4实验结果
已有研究表明，NMF具有很强的聚类能力，尤其是在文档聚类和图像聚类任务[42]、[37]中。它可以达到与大多数最先进的聚类算法类似或更好的性能，包括流行的光谱聚类方法[32]、[42]。
假设一个文档语料库由K个集群组成，每个集群对应一个连贯的主题。为了准确地对给定的文档语料库进行聚类，最好将文档投射到一个k维语义空间中，其中每个轴对应一个特定的主题[42]。在这个语义空间中，每个文档都可以表示为K个主题的线性组合。由于将每个文档看作基础主题的加法而不是减法混合更自然，所以组合系数都应该取非负值[42]。这些值可用于决定集群成员关系。在基于外观的视觉分析中，图像也可能与一些隐藏的部分相关联。例如，人脸图像可以被认为是鼻子、嘴巴、眼睛等的组合。要求组合系数为非负也是合理的。这是将NMF应用于文档和图像聚类的主要动机。在本节中，我们还评估了我们的GNMF算法在文档和图像聚类问题上的性能。
为了重现性，我们在以下位置提供代码和数据集:http://www.zjucadcg.cn/dengcai/GNMF/

4.1数据集
实验中使用了三个数据集。其中两个是图像数据集，第三个是文档语料库。这些数据集的重要统计数字摘要如下(又见表2):

。第一个数据集是COIL20图像库,其中包含32􏰈\*32灰度图像20从不同角度观察物体。
。第二个数据集是中央派面对数据库,其中包含32\*􏰈32灰度面对68人的图像。每个人在不同的光线和光照条件下有42张面部图像。
。第三个数据集是NIST主题检测与跟踪(TDT2)语料库。TDT2语料库由1998年上半年收集的6个来源的数据组成，包括两家通讯社(APW, NYT)、两家电台(VOA, PRI)和两家电视节目(CNN, ABC)。它由11,201个主题文件组成，分为96个类别。在这个实验中，删除了两类或两类以上的文档，只保留了最大的30个类别，因此总共留下了9394个文档。

4.2算法相比
为了证明我们的方法可以提高聚类性能，我们比较了以下五种常用的聚类算法:
。。
典型k均值聚类方法(简称k均值)。
主成分子空间(PCA)中的k -均值聚类。主成分分析(PCA)[24]是最著名的无监督降维算法之一。在主成分子空间中，聚类结构将更加明确。数学上讲，PCA相当于对中心数据矩阵执行SVD。在TDT2数据集中，我们简单地使用SVD而不是PCA，因为中心数据矩阵太大，无法装入内存。实际上，SVD已经成功地用于文档表示(潜在语义索引，[11])。有趣的是，Zha等人的[44]研究表明，SVD子空间中的K-means聚类与一种常用的谱聚类算法平均关联[38]有着密切的联系。他们发现，如果用内积来度量相似性并构造图，SVD后的k均值相当于平均关联。归一化割[38]是典型的光谱聚类算法(NCut)之一。
基于NMF的聚类(简称NMF)。我们使用f -范数公式，实现了NMF的归一化割加权版本，如[42]所示。我们在附录c中提供了NMF和GNMF的归一化割加权版本的简要描述，详情请参考[42]。基于f -范数公式的GNMF算法是本文提出的一种新算法。为了简便起见，我们使用0-1加权方法构造p近邻图。最近的邻居p的数量设置为5和正则化参数􏰁设置为100。参数选择和权重方案选择将在后面的小节中讨论。

![image-20190716195116663](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716195116663.png)

![image-20190716195125558](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716195125558.png)

![image-20190716195136726](/Users/daluzi/Library/Application Support/typora-user-images/image-20190716195136726.png)

在这五种算法中，NMF和GNMF可以学习基于部分的表示，因为它们只允许加减组合。NCut和GNMF两种方法考虑了数据的内在几何结构。
通过将每个样本的标签与数据集提供的标签进行比较，评估聚类结果。使用精确度(AC)和标准化互信息度量(NMI)两个指标来衡量聚类性能。有关这两个指标的详细定义，请参见[4]。

4.3聚类结果
表3、4和5分别显示了COIL20、PIE和TDT2数据集上的聚类结果。为了随机化实验，我们使用不同的聚类数进行评价。对于每个给定的集群号K，在随机选择的不同集群上进行20次测试运行(使用整个数据集的情况除外)。表中报告了性能的平均误差和标准误差。
这些实验揭示了一些有趣的观点:
。基于NMF和GNMF的方法都优于PCA (SVD)方法，说明了基于部件表示的优越性
发现隐藏因素的想法。
。NCut和GNMF算法都考虑了数据的几何结构，取得了比其他三种算法更好的性能。这说明了几何结构在学习中的重要性
隐藏的因素。
。无论数据集是什么，我们的GNMF总是获得最佳性能。这表明，利用基于部件的表示和图拉普拉斯正则化的能力，GNMF可以学习到更好的紧凑表示。

4.4参数选择
GNMF模型有两个重要参数:最近的邻居的数量􏰁p和正则化参数。无花果。1和2显示的平均性能GNMF􏰁和p随参数,分别。
我们可以看到,GNMF非常稳定的性能对参数􏰁。GNMF实现时考虑到——帐篷好性能􏰁从10到1000这三个数据集。
如前所述，GNMF使用p-nearest graph来捕捉数据分布的局部几何结构。

GNMF的成功依赖于两个相邻的数据点共享同一个标签的假设。显然，随着p的增加，这个假设更有可能失败。这就是GNMF性能随着p的增加而降低的原因，如图2所示。

4.5权重方案选择
如何在p近邻图上定义权矩阵W有多种选择。最常用的三种方法是0-1加权、热核加权和点积加权。在之前的实验中，为了简便起见，我们使用0-1加权。给定一个点x, 0-1权重对它的p个最近邻同等重要。然而，在很多情况下，有必要对这些p邻居进行微分，尤其是当p很大的时候。在这种情况下，可以使用热核加权或点积加权。
对于文本分析任务，通常将文档向量标准化为单元。在这种情况下，两个文档向量的点积变成了它们的余弦相似度，余弦相似度是信息检索领域中广泛使用的一种文档相似度度量方法。因此，对文本数据使用点积加权是非常自然的。与0-1加权类似，也没有用于点产品加权的参数。从图3可以看出，对于TDT2数据集上的点积和0-1加权方案，GNMF的性能都是最近邻p的函数。可以看出，点积加权优于0-1加权，尤其是当p较大时。对于点积加权，当p增加到23时，GNMF的性能仍然相当好，而当p增加(大于9时)，0-1加权时，GNMF的性能显著下降。

对于图像数据，一个合理的加权方案是热核加权。图4给出了热核数与COIL20数据集上0-1加权方案的最近邻p的函数性能。可以看出，热核数的加权也优于0-1加权，尤其是当p较大时。然而,有一个参数􏰀热内核中权重非常的关键性能。自动选择􏰀热内核中权重是一个具有挑战性的问题,收到了很多最近的研究兴趣。对这个问题的更详细的分析超出了本文的范围。有兴趣的读者可以参考[43]了解更多细节。

4.6收敛性研究
最小目标函数的更新规则本质上是迭代的。我们证明了这些规则是收敛的。在这里，我们研究这些规则的收敛速度。
图5为NMF和GNMF在三个数据集上的收敛曲线。对于每个图，y轴为目标函数值，x轴为迭代次数。我们可以看到，GNMF和NMF的乘法更新规则收敛得非常快，通常在100次迭代内。

4.7研究稀疏
NMF只允许基向量之间的加法组合，相信这一特性使NMF能够学习基于部件的表示[26]。最近的研究表明，NMF并不总是导致基于部分的表征[22]，[23]。一些研究人员通过合并U和/或V[23]上的稀疏约束来解决这个问题。在本节中，我们研究了在GNMF中学习的基向量的稀疏性。
图6和图7分别展示了NMF和GNMF在COIL20和PIE数据集中学习的基向量。每个基向量的维数为1024，具有单位范数。我们把这些基向量32􏰈32灰度图像。可以看出，GNMF学习的基向量比NMF学习的基向量稀疏。这一结果表明，与NMF相比，GNMF能更好地学习基于零件的表达。

5结论与未来工作
提出了一种新的矩阵分解方法，称为GNMF。GNMF将数据空间建模为嵌入到环境空间中的submani- fold，并在该流形上执行NMF。因此，与一般的只考虑数据欧氏结构的NMF方法相比，GNMF具有更强的识别能力。文档和图像聚类实验结果表明，GNMF在语义结构上具有较好的表示效果。
在我们未来的工作中还有几个问题需要研究:

1. 有一个参数􏰁控制GNMF模型的光滑,湖水。GNMF归结为原始NMF􏰁1⁄4 0时。因此,一个合适的价值􏰁我们的算法是至关重要的。目前还不清楚如何从理论上有效地进行模型选择。
2. NMF是对凸锥结构[14]的一种优化。保留角相似性的局域性可能更适合NMF框架，而不是用欧几里得方法保留闭合点的局域性。这为扩展NMF提供了另一种方法。
  3.我们的收敛证明本质上遵循李承宪和承宪对原始NMF的[27]证明的思想。对于F-norm公式，Lin[30]表明Lee和Seung的乘法算法不能保证收敛到一个固定点，并对Lee和Seung的算法进行了小的修改，可以收敛。我们在(14)和(15)中的更新规则本质上类似于NMF的更新规则。将Lin的思想应用到GNMF方法中是很有趣的。